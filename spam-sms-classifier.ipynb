{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y4-DBxqCDeeh"
   },
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iWo1GUs1q-KL",
    "outputId": "299de0e7-d8b9-4d7a-c8f3-faa7d4bbefaf"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from nltk.corpus import wordnet\n",
    "import subprocess\n",
    "\n",
    "# Download and unzip wordnet\n",
    "try:\n",
    "    nltk.data.find('wordnet.zip')\n",
    "except:\n",
    "    nltk.download('wordnet', download_dir='/kaggle/working/')\n",
    "    command = \"unzip /kaggle/working/corpora/wordnet.zip -d /kaggle/working/corpora\"\n",
    "    subprocess.run(command.split())\n",
    "    nltk.data.path.append('/kaggle/working/')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PtHK1Fxc5jRX"
   },
   "source": [
    "# Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vtGlvveT0MPd",
    "outputId": "8bd959ce-d777-42cc-b387-dc1f60fc95f3"
   },
   "outputs": [],
   "source": [
    "try_encodings = ['utf-8', 'latin-1', 'utf-16', 'ISO-8859-1']\n",
    "for encoding in try_encodings:\n",
    "    try:\n",
    "        df = pd.read_csv('/kaggle/input/sms-spam-collection-dataset/spam.csv', encoding=encoding)\n",
    "        print(f\"Successfully read the file with encoding: {encoding}\")\n",
    "        break\n",
    "    except UnicodeDecodeError:\n",
    "        print(f\"Failed to read with encoding: {encoding}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jBc8N6yX1Ylh",
    "outputId": "a0fd6e12-6fdd-4600-d023-0fd9a8684995"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tcA9iDeF5xum"
   },
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V9jPaSOA13Ir",
    "outputId": "6e0ea963-3d2f-4b24-d192-82f1b763c0ee"
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iAdOqFGA2BTB",
    "outputId": "e31bbf51-5ae7-4b42-ed4c-ad029a148031"
   },
   "outputs": [],
   "source": [
    "# checking missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V0M4ec-r22b4"
   },
   "outputs": [],
   "source": [
    "df.drop(['Unnamed: 2','Unnamed: 3','Unnamed: 4'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KQxXx4cU3Hk2",
    "outputId": "31448863-93d9-4a36-a351-aeafd860069a"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BF5UTuVt3P7l",
    "outputId": "e0a75069-3b8e-45f4-a0ee-9fd10c908d59"
   },
   "outputs": [],
   "source": [
    "# checking for duplicates values in the dataset\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T1_NE2cI3oGw"
   },
   "source": [
    "403 duplicated values found in the dataset. We need to remove them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b0Ndcwzj3tzW",
    "outputId": "1b01e546-1411-4f69-cf82-8f57addc5825"
   },
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3l-PBttu4Xj_",
    "outputId": "48bcdcbd-0d83-44e3-e6ab-bf30365e82e9"
   },
   "outputs": [],
   "source": [
    "# renmaing columns - giving columns meaningful names\n",
    "df.rename(columns={'v1':'label','v2':'message'},inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "997otjVT5V40",
    "outputId": "649a91c5-38cf-4fd8-a8af-ec7ec6921769"
   },
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(df['label'])\n",
    "le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6JqQz83l6Sxi"
   },
   "outputs": [],
   "source": [
    "df['y'] = le.transform(df.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "crBm2RPQ6hgB",
    "outputId": "0b879516-4115-4d1c-93e3-294f6ec33c25"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PJUPnDQ5CxV5"
   },
   "source": [
    "# Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OtLW5AVWCw0B",
    "outputId": "7376cf5c-bd4c-4bd1-c28a-86814fc2095e"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,4))\n",
    "values = df['label'].value_counts()\n",
    "plt.pie(values, labels=df.label.unique(), autopct='%1.2f%%',startangle=0,explode=(0,0.1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tZ3AKeUTa5SY",
    "outputId": "36634b60-9573-4c0a-c708-98e26c1636a3"
   },
   "outputs": [],
   "source": [
    "ax = sns.barplot(x=df.label.unique(),y=df['label'].value_counts(),)\n",
    "plt.ylabel('Count')\n",
    "\n",
    "for i, v in enumerate(df['label'].value_counts()):\n",
    "    ax.text(i, v+1, str(v), color='black', ha='center', va='bottom')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wczjYFHBj0YE"
   },
   "source": [
    "## Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9AjIAajajzfD"
   },
   "outputs": [],
   "source": [
    "wordnet = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NLH9O5ickCTK"
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # removing special characters & numbers and just keeping alphabets\n",
    "    var = re.sub(\"[^a-zA-Z0-9]\", \" \", text)\n",
    "\n",
    "    # lowercasing\n",
    "    var = var.lower()\n",
    "\n",
    "    var = var.split()\n",
    "    var = [wordnet.lemmatize(word) for word in var if not word in set(stopwords.words('english'))] # removing stopwords\n",
    "    var = ' '.join(var)\n",
    "\n",
    "    return var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8O1W5s3UkJpu",
    "outputId": "835f59fa-7025-4261-822f-9da66cce3e24"
   },
   "outputs": [],
   "source": [
    "print(df['message'][0])\n",
    "print('>>> After Processing')\n",
    "preprocess_text(df['message'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ePX_r2fHkB6U",
    "outputId": "edaec976-80a2-4cbb-ef9e-a88677ef90bc"
   },
   "outputs": [],
   "source": [
    "df['processed_message'] = df['message'].apply(preprocess_text)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "km5McSLWgqtq"
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "wc = WordCloud(width=300, height=300, min_font_size=10, background_color='white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wDp-pFqjhTFR",
    "outputId": "64ea971c-cc13-4f98-8357-3ef89abeea7c"
   },
   "outputs": [],
   "source": [
    "fig,axs = plt.subplots(nrows=1,ncols=2,figsize=(10,12))\n",
    "\n",
    "for i in range(len(df['label'].unique())):\n",
    "    ax = axs[i%2]\n",
    "    cloud = wc.generate(df[df['y'] == i]['processed_message'].str.cat(sep=\" \"))\n",
    "    ax.imshow(cloud)\n",
    "    ax.set_title(le.inverse_transform([i])[0])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vmxQbvaGj_JN",
    "outputId": "ba3a59e5-15bc-4d11-ce88-649fd728d8a5"
   },
   "outputs": [],
   "source": [
    "# Now building a corpus which will be a 2d list with 2 rows one row for each category (ham & spam)\n",
    "corpus = []\n",
    "for i in range(len(df['label'].unique())):\n",
    "    corpus_i = []\n",
    "    for desc in df[df['y'] == i]['processed_message'].tolist():\n",
    "        for word in desc.split():\n",
    "            corpus_i.append(word)\n",
    "    corpus.append(corpus_i)\n",
    "\n",
    "print(len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "occqthmSGXX_",
    "outputId": "33df2ae7-67f1-44a5-adae-c4c05e43a842"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "ham_df = pd.DataFrame(Counter(corpus[0]).most_common(40))\n",
    "ham_df.rename(columns={0:'word',1:'count'},inplace=True)\n",
    "ham_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yyGiyeckqHip",
    "outputId": "ce23bdd3-cd08-47e8-c6c5-1dad63ccd99a"
   },
   "outputs": [],
   "source": [
    "spam_df = pd.DataFrame(Counter(corpus[1]).most_common(40))\n",
    "spam_df.rename(columns={0:'word',1:'count'},inplace=True)\n",
    "spam_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5P4mmxpNqcWs",
    "outputId": "bde8b535-4a08-4e92-981b-a7ee0ae6a707"
   },
   "outputs": [],
   "source": [
    "sns.barplot(x=ham_df['word'],y=ham_df['count'])\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.title(\"Ham Word vs Count Plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jItQzaKEq5ui",
    "outputId": "c85c791b-12f0-4ac4-8692-a07989662ce6"
   },
   "outputs": [],
   "source": [
    "sns.barplot(x=spam_df['word'],y=spam_df['count'])\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.title(\"Spam Word vs Count Plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "90kdSlEkr8Yg"
   },
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GbsSF6d996dX"
   },
   "source": [
    "In order to see whether the length of the message has something to do in predicting the label or not. Let's add features like ```num_characters```, ```num_words``` and ```num_sentences``` to the dataframe. And then check the correlation of these features with the output label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HToRcNS8rDfn",
    "outputId": "d8bbc14f-cfe9-4209-fd35-add4c9e0b48b"
   },
   "outputs": [],
   "source": [
    "# Adding num_characters feature\n",
    "df['num_characters'] = df['message'].apply(len)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kNOnFvr53KEg",
    "outputId": "049789f0-d07c-4a2b-85ea-61f4361c83df"
   },
   "outputs": [],
   "source": [
    "df['num_words'] = df['message'].apply(lambda x: len(nltk.word_tokenize(x)))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p4j9z3J1mk1A",
    "outputId": "7a422d17-0e2e-4f66-e419-28ac2e6d2029"
   },
   "outputs": [],
   "source": [
    "df['num_sentences'] = df['message'].apply(lambda x: len(nltk.sent_tokenize(x)))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WnkyCCN84Fn3",
    "outputId": "4cdc7fb0-11d1-4b6b-8fb6-92f4d752af2a"
   },
   "outputs": [],
   "source": [
    "df[['num_characters','num_words','num_sentences']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "77hgc9p6TAiB",
    "outputId": "914eec82-e50a-4872-ecb1-d16986e4f060"
   },
   "outputs": [],
   "source": [
    "# ham\n",
    "df[df['y']==0][['num_characters','num_words','num_sentences']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oWbzjuSyTXJm",
    "outputId": "814c134b-f9c4-490e-c30f-b0d82120e37f"
   },
   "outputs": [],
   "source": [
    "# spam\n",
    "df[df['y']==1][['num_characters','num_words','num_sentences']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yBnQt-f1TkEP",
    "outputId": "ee91f911-6362-4e7e-e812-fd4762950a94"
   },
   "outputs": [],
   "source": [
    "sns.histplot(data=df,x='num_characters',hue='label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8_0HKmJvWR0x",
    "outputId": "0c159e17-4da1-47d4-ecef-bd071612e64a"
   },
   "outputs": [],
   "source": [
    "sns.histplot(data=df,x='num_words',hue='label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pIy0E1nrWUWP",
    "outputId": "3064d03d-5684-4644-ebf6-fa3a024ec967"
   },
   "outputs": [],
   "source": [
    "sns.histplot(data=df,x='num_sentences',hue='label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5ugo_aKeWptA",
    "outputId": "e0439890-c266-46a8-881f-3f1491eee7da"
   },
   "outputs": [],
   "source": [
    "sns.pairplot(data=df,hue='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qkyYcw2WYHQO",
    "outputId": "299d4b43-599c-4725-c660-ecf397794c37"
   },
   "outputs": [],
   "source": [
    "corr_mat = df.corr()\n",
    "sns.heatmap(corr_mat,annot=True,cmap='coolwarm',center=0)\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c9q40swbbhG9"
   },
   "source": [
    "From the above histogram plot of correlation matrix it is clear that ```num_characters``` is more related to output label ```y``` than ```num_words``` and ```num_sentences```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4zeSW7xoeeYu"
   },
   "source": [
    "# Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F0BKmmqpietG"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(max_features=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J0hZESnqieny",
    "outputId": "34a8cd7f-11b1-4c40-a03d-3d635cd9e734"
   },
   "outputs": [],
   "source": [
    "X = tfidf.fit_transform(df['processed_message']).toarray()\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EyT_IHDgiehg",
    "outputId": "208e0da8-24cc-43df-f691-38d39b23d1e8"
   },
   "outputs": [],
   "source": [
    "y = df['y'].values\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pMMHoR0kj-ZR"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IeWiay8VkLas"
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB,MultinomialNB,BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,precision_score,classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b_S5qHxWkP3N"
   },
   "outputs": [],
   "source": [
    "gnb = GaussianNB()\n",
    "mnb = MultinomialNB()\n",
    "bnb = BernoulliNB()\n",
    "lg = LogisticRegression()\n",
    "svc = SVC(kernel='sigmoid', gamma=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oCBexjSjkkxm"
   },
   "outputs": [],
   "source": [
    "clfs = {\n",
    "    'gnb':gnb,\n",
    "    'mnb':mnb,\n",
    "    'bnb':bnb,\n",
    "    'lg':lg,\n",
    "    'svc':svc\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "idR3NXMR2Ik2"
   },
   "outputs": [],
   "source": [
    "def train_clfs_and_predict(clfs,X_train,X_test,y_train,y_test):\n",
    "    acc = []\n",
    "    prec = []\n",
    "    conf_mat = []\n",
    "    classification_rep = []\n",
    "\n",
    "    for clf in clfs:\n",
    "        model = clfs[clf]\n",
    "        model.fit(X_train,y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        acc.append(accuracy_score(y_test,y_pred))\n",
    "        prec.append(precision_score(y_test,y_pred))\n",
    "        conf_mat.append(confusion_matrix(y_test,y_pred))\n",
    "        classification_rep.append(classification_report(y_test,y_pred))\n",
    "\n",
    "    return acc,prec,conf_mat,classification_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eIXjbYW72IfU",
    "outputId": "acf4df3d-e0b6-436c-9adb-a157ccd27ec7"
   },
   "outputs": [],
   "source": [
    "accuracy, precision, conf_mat, class_rep = train_clfs_and_predict(clfs,X_train,X_test,y_train,y_test)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XAPJ2HUv9NH5",
    "outputId": "9a6a9795-a1ed-496e-cd5f-0a8f249b07ca"
   },
   "outputs": [],
   "source": [
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hrHqmna-CAVR"
   },
   "outputs": [],
   "source": [
    "performance = {\n",
    "    'classifiers':list(clfs.keys()),\n",
    "    'accuracy':accuracy,\n",
    "    'precision':precision,\n",
    "    'confusion_matrix':conf_mat,\n",
    "    'classification_report':class_rep\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ae9kgFD2CTO-",
    "outputId": "8216c98c-7441-4618-9791-8f5dbf55d41b"
   },
   "outputs": [],
   "source": [
    "perf_df = pd.DataFrame(performance).sort_values(by='precision',ascending=False)\n",
    "perf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ov1WqQiaJDgC"
   },
   "source": [
    "While doing feature engineering, we noticed that ```num_characters``` had a high correlation with the label ```y```. So let's try adding this feature and then check if the performance increases or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gW3EcxfnJDML",
    "outputId": "76fcf447-9eb2-4e0d-86d0-b338518dd074"
   },
   "outputs": [],
   "source": [
    "num_chars = df.num_characters.values.reshape(-1,1)\n",
    "print(\"num_chars Min: \",np.min(num_chars))\n",
    "print(\"num_chars Max: \",np.max(num_chars),\"\\n\")\n",
    "print(f\"X min: {np.min(X)}\")\n",
    "print(f\"X max: {np.max(X)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qiGjEl9CKg4Z"
   },
   "source": [
    "From above cell we see that we need to scale ```num_chars``` and then add it to feature matrix ```X```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "16r-ZirwPfNE"
   },
   "outputs": [],
   "source": [
    "# Not using StandardScaler because it gives both -ve and +ve values and MultinomialNB doesn't accept -ve values. So we're using MinMaxScaler\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pgyaT4EmPlO9",
    "outputId": "ba3af26e-8fed-44b8-84a5-d753d7285268"
   },
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaled_num_chars = scaler.fit_transform(num_chars)\n",
    "\n",
    "# now check the min and max values\n",
    "print(\"scaled_num_chars Min: \",np.min(scaled_num_chars))\n",
    "print(\"scaled_num_chars Max: \",np.max(scaled_num_chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jFws__CyWNjc"
   },
   "outputs": [],
   "source": [
    "X = np.hstack((X,scaled_num_chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OIUadkKkW2r6",
    "outputId": "0a2a529c-b84d-4787-a35e-20b68e80dca6"
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yhKdu0iPWvEn"
   },
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DAUqo765WtdC",
    "outputId": "70ecfb4f-b77c-445f-ad81-4f6c625c10c1"
   },
   "outputs": [],
   "source": [
    "accuracy1, precision1, conf_mat1, class_rep1 = train_clfs_and_predict(clfs,X_train,X_test,y_train,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RrzE4v4vXF-U"
   },
   "outputs": [],
   "source": [
    "perf_df['num_chars_accuracy'] = accuracy1\n",
    "perf_df['num_chars_percision'] = precision1\n",
    "perf_df['num_chars_confusion_matrix'] = conf_mat1\n",
    "perf_df['num_chars_classification_report'] = class_rep1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "trrlwWciWMwx",
    "outputId": "c821974d-8ca5-4018-9f8f-c0ddd55b6c54"
   },
   "outputs": [],
   "source": [
    "perf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jt2B9kcxYz56"
   },
   "source": [
    "Adding ```num_characters``` feature didn't help. Almost every classifier performed even more worse. So, we won't use this feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B1LN7wzoiead",
    "outputId": "f9e49f8e-dd6e-4b99-dbd2-18cde2772e34"
   },
   "outputs": [],
   "source": [
    "cols_to_extract = ['classifiers','accuracy','precision','confusion_matrix','classification_report']\n",
    "final_performance_df = perf_df[cols_to_extract]\n",
    "final_performance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BTOFTTnHj-QN",
    "outputId": "02e340ef-a074-4671-9804-3912abcc19a5"
   },
   "outputs": [],
   "source": [
    "# printing out classification reports\n",
    "reports = perf_df.classification_report.values\n",
    "classifiers = perf_df.classifiers.values\n",
    "\n",
    "for i,clf in enumerate(classifiers):\n",
    "    print(f\"{clf}:\\n{reports[i]}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m-G1RHZAa04X",
    "outputId": "3167c9be-4b4b-438e-9d7a-d8d7df72c876"
   },
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\")\n",
    "sns.lineplot(x=final_performance_df.classifiers, y=final_performance_df.accuracy, marker='o', label='Accuracy', data=final_performance_df)\n",
    "sns.lineplot(x=final_performance_df.classifiers, y=final_performance_df.precision, marker='o', label='Precision', data=final_performance_df)\n",
    "\n",
    "plt.title(\"Accuracy and Precision by Classifiers\")\n",
    "plt.xlabel(\"Classifiers\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HaGeJdA1c8Nx"
   },
   "source": [
    "**RESULT**\n",
    "\n",
    "The above plot shows that svc has the best performance with ```accuracy = 98.06%``` and ```precision = 100%```\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
